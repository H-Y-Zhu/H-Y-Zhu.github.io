<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
        "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
    <meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/"/>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8"/>
    <link rel="stylesheet" href="jemdoc.css" type="text/css"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <title>Research Projects</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
    <tr valign="top">
        <td id="layout-menu">
            <div class="menu-category">Home</div>
            <div class="menu-item"><a href="index.html">About&nbsp;me</a></div>
            <div class="menu-category">Research</div>
            <div class="menu-item"><a href="publication.html">Publications</a></div>
            <div class="menu-item"><a href="project.html" class="current">Projects</a></div>
        </td>
        <td id="layout-content">
            <div id="toptitle">
                <h1>Research Projects</h1>
            </div>
            <p>
                Interested local/international students can apply through ASTAR scholarships, such as
                <a
                        href="https://www.a-star.edu.sg/Scholarships/for-graduate-studies/a-star-graduate-scholarship-singapore"
                        target="_blank">
                    AGS</a> (PhD),
                <a
                        href="https://www.a-star.edu.sg/Scholarships/for-graduate-studies/singapore-international-graduate-award-singa"
                        target="_blank">
                    SINGA</a> (PhD),
                <a
                        href="https://www.a-star.edu.sg/Scholarships/for-undergraduate-studies/singapore-international-pre-graduate-award-sipga"
                        target="_blank">
                    SIPGA</a> (Master and Undergraduate), or explore other attachment and internship opportunities by
                directly contacting me at <a href="mailto:zhu_haiyue@simtech.a-star.edu.sg"><i>zhu_haiyue@simtech.a-star.edu.sg</i></a>
                with your CV.
            </p>
            <h2>Data-Efficient Robotics Foundation Models for Intelligent Perception and Reasoning</h2>
            <ul>
                <li>
                    <p>This research aims to develop data-efficient intelligent vision perception and reasoning for
                        smart robotics using foundation models and advanced learning techniques. Autonomous robots
                        require
                        robust perception and reasoning capabilities to understand complex environments and make
                        real-time
                        decisions for autonomous and semi-autonomous operations. However, real-world robotics
                        applications
                        face data scarcity, domain shifts, and the need for flexible deployment, making traditional
                        big-data-driven deep learning approaches impractical. This research will focus on enhancing
                        sample
                        efficiency in robotics AI using techniques such as:</p>
                    <ul>
                        <p>
                            • Foundation Models for Robotics – leveraging pretrained vision-language models to improve
                            generalization across diverse tasks.
                        </p>
                        <p>
                            • Few-Shot and Self-Supervised Learning – reducing labeled data dependency while enabling
                            robots
                            to
                            adapt to new environments.
                        </p>
                        <p>
                            • Neuro-Symbolic AI for Perception and Reasoning – integrating deep learning with symbolic
                            reasoning
                            for better interpretability and decision-making.
                        </p>
                        <p>
                            • Physics-Informed AI and Sim-to-Real Learning – improving model robustness and adaptability
                            in
                            real-world scenarios.
                        </p>
                    </ul>
                    <p>
                        The applications include robotic perception and reasoning tasks for complex scene understanding
                        to
                        afford the autonomous robot manipulation. The research outcomes will significantly benefit areas
                        like autonomous manufacturing and human-robot collaboration in unstructured environments,
                        aligning
                        with Singapore’s vision of a Smart Nation.
                    </p>
                </li>
            </ul>
            <h2>Spatial-Aware Vision-Language-Action Models for Embodied AI in Robotics</h2>
            <ul>
                <li><p>Spatial awareness is crucial for Vision-Language-Action (VLA) models in robotics, enabling
                    intelligent agents to understand, navigate, and interact with their environment effectively.
                    Traditional VLA models primarily focus on multimodal understanding but often lack precise spatial
                    reasoning, limiting their ability to ground language in 3D space and execute physical actions. This
                    research focuses on developing spatial-aware vision-language-action (VLA) models to enhance embodied
                    AI for intelligent robotics. Future autonomous systems require multimodal reasoning to understand
                    their environment, process natural language instructions, and execute complex actions with spatial
                    awareness.
                </p>
                    <p>
                        Key challenges include aligning spatial perception with language grounding, understanding
                        geometric
                        and semantic relationships, and adapting to novel environments with minimal supervision. This
                        research will explore:
                    <ul>
                        </p>
                        • Vision-Language-Action Foundation Models – leveraging large-scale pretrained models for
                        cross-modal understanding.
                        </p>
                        <p>
                            • Spatial-Temporal Scene Understanding – integrating 3D spatial reasoning and video-language
                            models
                            for dynamic environments.
                        </p>
                        <p>
                            • 3D Spatial Grounding – Linking natural language commands to 3D world representations for
                            precise
                            execution.
                        </p>
                        <p>
                            • Spatial-Temporal Reasoning – Understanding motion, occlusion, and depth for dynamic
                            interactions.
                        </p>
                        <p>
                            • Sim-to-Real Transfer for Spatially-Aware Interaction – improving real-world adaptability
                            through
                            simulation-driven learning.
                        </p>
                    </ul>
                    <p>
                        Applications include autonomous robots, assistive robotics, and human-robot collaboration
                        in industrial and service environments. The research outcomes will contribute to next-generation
                        robotic agents capable of understanding spatial contexts and executing human-like actions in
                        unstructured settings.
                    </p>
                </li>
            </ul>
            <div id="footer">
                <div id="footer-text">
                    Page generated 2021-12-09 00:01:33 Malay Peninsula Standard Time, by <a
                        href="http://jemdoc.jaboc.net/">jemdoc</a>.
                </div>
            </div>
        </td>
    </tr>
</table>
</body>
</html>
